{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.__version__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "#import keras\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "\n",
    "from skimage.measure import compare_psnr, compare_ssim\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Input, Conv2D, Activation, BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.layers.core import Dropout\n",
    "#import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.engine import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Conv2D, Activation, BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "from keras.layers import Input, Activation, Add\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "image_shape = (448,896,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def res_block(input, filters, kernel_size=(3,3), strides=(1,1), use_dropout=False):\n",
    "    \"\"\"\n",
    "    Instanciate a Keras Resnet Block using sequential API.\n",
    "    :param input: Input tensor\n",
    "    :param filters: Number of filters to use\n",
    "    :param kernel_size: Shape of the kernel for the convolution\n",
    "    :param strides: Shape of the strides for the convolution\n",
    "    :param use_dropout: Boolean value to determine the use of dropout\n",
    "    :return: Keras Model\n",
    "    \"\"\"\n",
    "    x = ReflectionPadding2D((1,1))(input)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if use_dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ReflectionPadding2D((1,1))(x)\n",
    "    x = Conv2D(filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Two convolution layers followed by a direct connection between input and output\n",
    "    merged = Add()([input, x])\n",
    "    \n",
    "    return merged\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    \"\"\"Reflection-padding layer for 2D input (e.g. picture).\n",
    "    This layer can add rows and columns or zeros\n",
    "    at the top, bottom, left and right side of an image tensor.\n",
    "    # Arguments\n",
    "        padding: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n",
    "            - If int: the same symmetric padding\n",
    "                is applied to width and height.\n",
    "            - If tuple of 2 ints:\n",
    "                interpreted as two different\n",
    "                symmetric padding values for height and width:\n",
    "                `(symmetric_height_pad, symmetric_width_pad)`.\n",
    "            - If tuple of 2 tuples of 2 ints:\n",
    "                interpreted as\n",
    "                `((top_pad, bottom_pad), (left_pad, right_pad))`\n",
    "        data_format: A string,\n",
    "            one of `channels_last` (default) or `channels_first`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `channels_last` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `channels_first`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        - If `data_format` is `\"channels_last\"`:\n",
    "            `(batch, rows, cols, channels)`\n",
    "        - If `data_format` is `\"channels_first\"`:\n",
    "            `(batch, channels, rows, cols)`\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        - If `data_format` is `\"channels_last\"`:\n",
    "            `(batch, padded_rows, padded_cols, channels)`\n",
    "        - If `data_format` is `\"channels_first\"`:\n",
    "            `(batch, channels, padded_rows, padded_cols)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 padding=(1, 1),\n",
    "                 data_format=None,\n",
    "                 **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.K.normalize_data_format(data_format)\n",
    "        if isinstance(padding, int):\n",
    "            self.padding = ((padding, padding), (padding, padding))\n",
    "        elif hasattr(padding, '__len__'):\n",
    "            if len(padding) != 2:\n",
    "                raise ValueError('`padding` should have two elements. '\n",
    "                                 'Found: ' + str(padding))\n",
    "            height_padding = conv_utils.normalize_tuple(padding[0], 2,\n",
    "                                                        '1st entry of padding')\n",
    "            width_padding = conv_utils.normalize_tuple(padding[1], 2,\n",
    "                                                       '2nd entry of padding')\n",
    "            self.padding = (height_padding, width_padding)\n",
    "        else:\n",
    "            raise ValueError('`padding` should be either an int, '\n",
    "                             'a tuple of 2 ints '\n",
    "                             '(symmetric_height_pad, symmetric_width_pad), '\n",
    "                             'or a tuple of 2 tuples of 2 ints '\n",
    "                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n",
    "                             'Found: ' + str(padding))\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            if input_shape[2] is not None:\n",
    "                rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[3] is not None:\n",
    "                cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    rows,\n",
    "                    cols)\n",
    "        elif self.data_format == 'channels_last':\n",
    "            if input_shape[1] is not None:\n",
    "                rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n",
    "            else:\n",
    "                rows = None\n",
    "            if input_shape[2] is not None:\n",
    "                cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n",
    "            else:\n",
    "                cols = None\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return spatial_reflection_2d_padding(inputs,\n",
    "                                             padding=self.padding,\n",
    "                                             data_format=self.data_format)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "def spatial_reflection_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n",
    "    \"\"\"\n",
    "    Pad the 2nd and 3rd dimensions of a 4D tensor.\n",
    "    :param x: Input tensor\n",
    "    :param padding: Shape of padding to use\n",
    "    :param data_format: Tensorflow vs Theano convention ('channels_last', 'channels_first')\n",
    "    :return: Tensorflow tensor\n",
    "    \"\"\"\n",
    "    assert len(padding) == 2\n",
    "    assert len(padding[0]) == 2\n",
    "    assert len(padding[1]) == 2\n",
    "    if data_format is None:\n",
    "        data_format = image_data_format()\n",
    "    if data_format not in {'channels_first', 'channels_last'}:\n",
    "        raise ValueError('Unknown data_format ' + str(data_format))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        pattern = [[0, 0],\n",
    "                   [0, 0],\n",
    "                   list(padding[0]),\n",
    "                   list(padding[1])]\n",
    "    else:\n",
    "        pattern = [[0, 0],\n",
    "                   list(padding[0]), list(padding[1]),\n",
    "                   [0, 0]]\n",
    "    return tf.pad(x, pattern, \"REFLECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from keras.layers import ReflectionPadding2D, res_block\n",
    " # Input shape\n",
    "channels = 1\n",
    "lr_height = 448                 # Low resolution height\n",
    "lr_width = 896                 # Low resolution width\n",
    "lr_shape = (lr_height, lr_width,  channels)\n",
    " \n",
    "\n",
    "# Number of residual blocks in the generator\n",
    "n_residual_blocks = 16\n",
    "\n",
    " \n",
    "gf = 64\n",
    "\n",
    "def residual_block(layer_input, filters):\n",
    "    d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "    d = Activation('relu')(d)\n",
    "    d = BatchNormalization(momentum=0.8)(d)\n",
    "    d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "    d = BatchNormalization(momentum=0.8)(d)\n",
    "    d = Add()([d, layer_input])\n",
    "    return d\n",
    "\n",
    "def deconv2d(layer_input):\n",
    "    u = UpSampling2D(size=2)(layer_input)\n",
    "    u = Conv2D(256, kernel_size=3, strides=1, padding='same')(u)\n",
    "    u = Activation('relu')(u)\n",
    "    return u\n",
    "    \n",
    "def generator_model():\n",
    "     \n",
    "\n",
    "        # Low resolution image input\n",
    "    img_lr = Input(shape= lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "    c1 = Conv2D(64, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "    c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "    r = residual_block(c1, self.gf)\n",
    "    for _ in range(self.n_residual_blocks - 1):\n",
    "        r = residual_block(r, self.gf)\n",
    "\n",
    "        # Post-residual block\n",
    "    c2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
    "    c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "    c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "    u1 = deconv2d(c2)\n",
    "    u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "    gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "\n",
    "    return Model(img_lr, gen_hr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ndf = 64\n",
    "output_nc = 1\n",
    "input_shape_discriminator = (448, 896, output_nc)\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    \"\"\"Build discriminator architecture.\"\"\"\n",
    "    n_layers, use_sigmoid = 3, False\n",
    "    inputs = Input(shape=input_shape_discriminator)\n",
    "\n",
    "    x = Conv2D(filters=ndf, kernel_size=(4,4), strides=2, padding='same')(inputs)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult, nf_mult_prev = 1, 1\n",
    "    for n in range(n_layers):\n",
    "        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
    "        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
    "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    if use_sigmoid:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='tanh')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_images = generator(inputs)\n",
    "    outputs = discriminator(generated_images)\n",
    "    model = Model(inputs=inputs, outputs=[generated_images, outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from  keras.layers import Concatenate\n",
    "\n",
    "\n",
    "image_shape = (448,896,1)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False,   input_shape=(512,896,3) )\n",
    "    y_true_c =   Concatenate()([y_true,y_true,y_true])    \n",
    "    y_pred_c =   Concatenate()([y_pred,y_pred,y_pred])    \n",
    "\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    loss_model.trainable = False\n",
    "    return K.mean(K.square(loss_model(y_true_c) - loss_model(y_pred_c)))\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing images ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cbe7d361c94e46a221647c0bfcff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rupali/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image/utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Set some parameters\n",
    "im_width = 896\n",
    "im_height = 448\n",
    "border = 5\n",
    "path_train = 'train/'\n",
    "path_test = 'test/'\n",
    "\n",
    "def get_data(path, train=True):\n",
    "    ids = next(os.walk(path + \"raw\"))[2]\n",
    "    \n",
    "    X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "    if train:\n",
    "        y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "    print('Getting and resizing images ... ')\n",
    "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
    "        # Load raw\n",
    "        img = load_img(path + 'raw/' + id_, grayscale=True)\n",
    "        x_img = img_to_array(img)\n",
    "        x_img = resize(x_img, (448,896, 1), mode='constant', preserve_range=True)\n",
    "\n",
    "        # Load average\n",
    "        if train:\n",
    "            average = img_to_array(load_img(path + 'average/' + id_, grayscale=True))\n",
    "            average = resize(average, (448, 896, 1), mode='constant', preserve_range=True)\n",
    "\n",
    "        # Save images\n",
    "        X[n, ..., 0] = x_img.squeeze() / 255\n",
    "        if train:\n",
    "            y[n] = average / 255\n",
    "    print('Done!')\n",
    "    if train:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "x_train, y_train = get_data(path_train, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4086624110808043 386.82877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|▍         | 1/25 [01:30<36:06, 90.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/25 [02:03<28:00, 73.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4927364721894264 203.28513\n",
      "-0.49826987624168395 177.99585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█▏        | 3/25 [02:39<22:47, 62.16s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import click\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def save_all_weights(d, g, epoch_number, current_loss):\n",
    "    now = datetime.datetime.now()\n",
    "    save_dir = os.path.join('GAN/4_RES/', '{}{}'.format(now.month, now.day))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(epoch_number, current_loss)), True)\n",
    "    d.save_weights(os.path.join(save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)\n",
    "\n",
    "\n",
    "def train_multiple_outputs(n_images, batch_size, log_dir, epoch_num, critic_updates=5):\n",
    " \n",
    "    g = generator_model()\n",
    "    #g =build_res_unet()\n",
    "    #g.load_weights('GAN/327/generator_9_86.h5')\n",
    "\n",
    "    d = discriminator_model()\n",
    "    d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n",
    "\n",
    "    d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    d.trainable = True\n",
    "    d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
    "    d.trainable = False\n",
    "    loss = [perceptual_loss, wasserstein_loss]\n",
    "    loss_weights = [100, 1]\n",
    "    d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "    d.trainable = True\n",
    "\n",
    "    output_true_batch, output_false_batch = np.ones((batch_size, 1)), -np.ones((batch_size, 1))\n",
    "\n",
    "    log_path = './logs'\n",
    "    tensorboard_callback = TensorBoard(log_path)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epoch_num)):\n",
    "        permutated_indexes = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "        d_losses = []\n",
    "        d_on_g_losses = []\n",
    "        for index in range(int(x_train.shape[0] / batch_size)):\n",
    "            batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
    "            image_blur_batch = x_train[batch_indexes]\n",
    "            image_full_batch = y_train[batch_indexes]\n",
    "\n",
    "            generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "\n",
    "            for _ in range(critic_updates):\n",
    "                d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
    "                d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "                d_losses.append(d_loss)\n",
    "\n",
    "            d.trainable = False\n",
    "\n",
    "            d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
    "            d_on_g_losses.append(d_on_g_loss)\n",
    "\n",
    "            d.trainable = True\n",
    "\n",
    "        # write_log(tensorboard_callback, ['g_loss', 'd_on_g_loss'], [np.mean(d_losses), np.mean(d_on_g_losses)], epoch_num)\n",
    "        print(np.mean(d_losses), np.mean(d_on_g_losses))\n",
    "        with open('log.txt', 'a+') as f:\n",
    "            f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
    "\n",
    "        save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))\n",
    "    \n",
    "\n",
    "train_multiple_outputs(n_images=40, batch_size=1, log_dir=\"lg\", epoch_num=25, critic_updates=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing images ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9cf22043294ee78c6148717697666d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rupali/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image/utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "im_width = 896\n",
    "im_height = 448\n",
    "border = 5\n",
    "path_train = 'train/'\n",
    "path_test = 'test/'\n",
    "\n",
    "def get_data(path, train=True):\n",
    "    ids = next(os.walk(path + \"raw\"))[2]\n",
    "    \n",
    "    X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "    if train:\n",
    "        y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "    print('Getting and resizing images ... ')\n",
    "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
    "        # Load raw\n",
    "        img = load_img(path + 'raw/' + id_, grayscale=True)\n",
    "        x_img = img_to_array(img)\n",
    "        x_img = resize(x_img, (448,896, 1), mode='constant', preserve_range=True)\n",
    "\n",
    "        # Load average\n",
    "        if train:\n",
    "            average = img_to_array(load_img(path + 'average/' + id_, grayscale=True))\n",
    "            average = resize(average, (448, 896, 1), mode='constant', preserve_range=True)\n",
    "\n",
    "        # Save images\n",
    "        X[n, ..., 0] = x_img.squeeze() / 255\n",
    "        if train:\n",
    "            y[n] = average / 255\n",
    "    print('Done!')\n",
    "    if train:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "x_test, y_test = get_data(path_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rupali/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/home/rupali/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/home/rupali/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.004141562240196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import click\n",
    "import scipy.misc\n",
    "#from scipy.stats import signaltonoise\n",
    "p=[]\n",
    "def test(batch_size):\n",
    "    g = generator_model()\n",
    "    #g =build_res_unet()\n",
    "\n",
    "    g.load_weights('GAN/410/generator_24_86.h5')\n",
    "    generated_images = g.predict(x=x_test, batch_size=batch_size)\n",
    "    #generated = np.array([deprocess_image(img) for img in generated_images])\n",
    "    #x_test = deprocess_image(x_test)\n",
    "    #y_test = deprocess_image(y_test)\n",
    "   \n",
    "    for i in range(generated_images.shape[0]):\n",
    "        y = y_test[i, :, :, :]\n",
    "        x = x_test[i, :, :, :]\n",
    "        img = generated_images[i, :, :, :]\n",
    "        output = np.concatenate((y, x, img), axis=1)\n",
    "        scipy.misc.imsave('GAN/SBSDI_results/results{}.png'.format(i+1),img[:,:,0])\n",
    "        scipy.misc.imsave('GAN/SBSDI_results/raw{}.png'.format(i+1),x[:,:,0])\n",
    "        scipy.misc.imsave('GAN/SBSDI_results/average{}.png'.format(i+1),y[:,:,0])\n",
    "\n",
    "        p.append(compare_psnr(img[:,:,0],y[:,:,0]))\n",
    "        #snr = signaltonoise(img[:,:,0])\n",
    "        #print(p)\n",
    "    print(np.array(p).mean())\n",
    " \n",
    "\n",
    "        #im = Image.fromarray(output.astype(np.uint8))\n",
    "        #im.save('results{}.png'.format(i))\n",
    "\n",
    "test(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.270462589881618, 23.755853416680534, 23.067726741157337, 27.53470119858962, 32.19994124182078, 26.024050720067045, 22.309713476835324, 22.549416221351976, 21.439642824429257, 32.501234359969274, 27.691091340632347, 27.749814430998274, 24.540039674665394, 26.754397352407615, 28.017482773832324, 26.53797653271586, 27.671309043976052, 25.7199477922957]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
